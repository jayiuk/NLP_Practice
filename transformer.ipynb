{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scaled dot product attention\n",
    "  - multi-head attention의 핵심적인 부분\n",
    "  - query, key의 transpose의 matmul(dot product) -> similarity를 계산\n",
    "  - 이 similarity를 기반으로 value값을 참조\n",
    "  - decoder 부분에서는 masking처리를 해야함 -> if문으로 mask부분 포함해서 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(self, q, k, v, mask = None):\n",
    "    d_k = k.size()[-1]\n",
    "    k_transpose = torch.transpose(k, 3, 2)\n",
    "    output = torch.matmul(q, k_transpose)\n",
    "    output = output / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        output = output.masked_fill(mask.unsqueeze(1).unsqueeze(-1), 0)\n",
    "    output = F.softmax(output, -1)\n",
    "    output = torch.matmul(output, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi-Head Attention\n",
    "  - scaled dot-product attention을 query에 해당하는 value 값들을 참조하기 위해 사용\n",
    "    - 이 때 query, key, value를 그대로 사용안함\n",
    "    - 여러개의 head로 나누고 query, key, value를 linear projection한 후 사용\n",
    "  - scaled dot-product attention 이후 각 head의 value값을 concat -> linear layer -> output\n",
    "  - contiguous : sequence의 순서가 중요 -> 순서 유지를 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_num = 512, head_num = 8):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.dim_num = dim_num\n",
    "        self.query_embed = nn.Linear(dim_num, dim_num)\n",
    "        self.key_embed = nn.Linear(dim_num, dim_num)\n",
    "        self.value_embed = nn.Linear(dim_num, dim_num)\n",
    "        self.output_embed = nn.Linear(dim_num, dim_num)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask = None):\n",
    "        d_k = k.size()[-1]\n",
    "        k_transpose = torch.transpose(k, 3, 2)\n",
    "        output = torch.matmul(q, k_transpose)\n",
    "        output = output / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            output = output.masked_fill(mask.unsqueeze(1).unsqueeze(-1), 0)\n",
    "        output = F.softmax(output, -1)\n",
    "        output = torch.matmul(output, v)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        batch_size = q.size()[0]\n",
    "        q = self.query_embed(q).view(batch_size, -1, self.head_num, self.dim_num // self.head_num).transpose(1, 2)\n",
    "        k = self.key_embed(k).view(batch_size, -1, self.head_num, self.dim_num // self.head_num).transpose(1, 2)\n",
    "        v = self.value_embed(v).view(batch_size, -1, self.head_num, self.dim_num//self.head_num).transpose(1, 2)\n",
    "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        batch_num, head_num, seq_num, hidden_num = output.size()\n",
    "        output = torch.transpose(output, 1, 2).contiguous().view((batch_size, -1, hidden_num * self.head_num))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layer norm\n",
    "  - dimension layer 방향으로 평균을 빼고 표준편차로 나누는 Normalization 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(self, input):\n",
    "    mean = torch.mean(input, dim = -1, keepdim = True)\n",
    "    std = torch.std(input, dim = -1, keepdim = True)\n",
    "    output = (input - mean) / std\n",
    "    return output\n",
    "\n",
    "# torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine = True, bias = True, device = None, dtype = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add & LayerNorm\n",
    "  - 이전 층의 output을 layer norm을 통해 normalization한 후 residual값을 더해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def layer_norm(self, input):\n",
    "        mean = torch.mean(input, dim = -1, keepdim = True)\n",
    "        std = torch.std(input, dim = -1, keepdim = True)\n",
    "        output = (input - mean) / std\n",
    "        return output\n",
    "\n",
    "    def forward(self, input, residual):\n",
    "        return residual + self.layer_norm(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feed Forward\n",
    "  - Fully Connected Layer -> ReLU -> Fully Connected Layer로 구성되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_num = 512):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(dim_num, dim_num *4)\n",
    "        self.layer2 = nn.Linear(dim_num*4, dim_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.layer1(input)\n",
    "        output = self.layer2(F.relu(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder\n",
    "  - Multi-Head Attention -> Residual Add & Layer Norm -> Feed Forward -> Residual Add & Layer Norm\n",
    "  - sublayer들을 연결하는 방식으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_num = 512):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHeadAttention(dim_num = dim_num)\n",
    "        self.addlayernorm = AddLayerNorm()\n",
    "        self.feedforward = FeedForward(dim_num = dim_num)\n",
    "        self.layer2 = AddLayerNorm()\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        multihead_output = self.multihead(q, k, v)\n",
    "        residual1_output = self.addlayernorm(multihead_output, q)\n",
    "        feedforward_output = self.feedforward(residual1_output)\n",
    "        output = self.layer2(feedforward_output, residual1_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoder\n",
    "  - Masked Multi-Head Attention -> Residual Add & Layer Norm -> Multi-Head Attention -> Residual Add & Layer Norm -> Feed Forward -> Residual Add & Layer Norm\n",
    "  - Encoder와 비슷하게 sublayer들을 연결하는 방식으로 구현\n",
    "    - 중간 Multi-Head Attention은 query, key를 Encoder의 Output을 사용 -> 이 점을 명시해야 함\n",
    "  - masking을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_num = 512):\n",
    "        super().__init__()\n",
    "        self.masked_multihead = MultiHeadAttention(dim_num = dim_num)\n",
    "        self.residual1 = AddLayerNorm()\n",
    "        self.multihead = MultiHeadAttention(dim_num = dim_num)\n",
    "        self.residual2 = AddLayerNorm()\n",
    "        self.feed_forward = FeedForward(dim_num = dim_num)\n",
    "        self.residual3 = AddLayerNorm()\n",
    "\n",
    "    def forward(self, o_q, o_k, o_v, encoder_output, mask):\n",
    "        masked_output = self.masked_multihead(o_q, o_k, o_v, mask)\n",
    "        res1_output = self.residual1(masked_output, o_q)\n",
    "        multihead_output = self.multihead(encoder_output, encoder_output, res1_output, mask)\n",
    "        res2_output = self.residual2(multihead_output, res1_output)\n",
    "        feedforward_output = self.feed_forward(res2_output)\n",
    "        output = self.residual3(feedforward_output, res2_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- positional_encoding\n",
    "  - 짝수번째 token과 홀수번째 token이 각기 다른 식을 따름\n",
    "    - i = hidden dimension 방향의 인덱스, pos = postional 방향(몇 번째 seq인지)\n",
    "  - 크게 두 부분에서 사용됨\n",
    "    - input, output의 sequence length길이가 다를 수 있음\n",
    "      - 이것을 인자로 받게 구성\n",
    "  - self.register_buffer : model parameter 학습 시 positional encoding이 학습되지 않도록 막아주기 위한 용도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PE_{(pos, 2i)} = sin(pos/10000^{21/d_{model}}) \\\\ PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(self, position_max_length = 100):\n",
    "    position = torch.arange(0, position_max_length, dtype = torch.float).unsqueeze(1)\n",
    "    pe = torch.zeros(position_max_length, self.hidden_dim)\n",
    "    div_term = torch.pow(torch.ones(self.hidden_dim // 2).fill_(10000), torch.arange(0, self.hidden_dim, 2) / torch.tensor(self.hidden_dim, dtype = torch.float32))\n",
    "    pe[:, 0::2] = torch.sin(position/div_term)\n",
    "    pe[:, 1::2] = torch.cos(position / div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input & output embedding\n",
    "  - nn.Embedding을 통해 쉽게 구현 가능\n",
    "  - 첫번째 인자 : input 데이터의 total word 개수, 두 번째 인자는 hidden dimension의 수\n",
    "  - total_word_num : sequence dictionary에 존재하는 unique value의 개수(전체 단어 개수 아님)\n",
    "    - task에 따라 input, output의 단어의 개수가 다를 수 있어 다른 인자를 받아야함(번역)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.input_data_embedding = nn.Embedding(total_word_num, self.hidden_dim)\n",
    "# self.output_data_embedding = nn.Embedding(total_word_num, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer\n",
    "  - Encoder 부분은 앞서 구현했던 Encoder를 n번 반복\n",
    "  - Encoder 부분에 들어가는 query, key, value는 문장의 embedding한 값으로 모두 같고, 전번째 encoder의 결과가 다음 encoder의 query, key, value가 됨\n",
    "  - Decoder 부분도 비슷\n",
    "    - Encoder의 output이 사용 된다는 점, Decoder에서는 다음 sequence를 볼 수 없음 -> mask를 사용해서\n",
    "    - 이 두가지 차이가 존재\n",
    "  - 실제 학습에서 Decoder에 masking을 매우 작은 값을 사용하는게 유리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_num = 6, decoder_num = 6, hidden_dim = 512, max_encoder_seq_len = 100, max_decoder_seq_len = 100):\n",
    "        super().__init__()\n",
    "        self.encoder_num = encoder_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder_num = decoder_num\n",
    "        self.max_encoder_seq_len = max_encoder_seq_len\n",
    "        self.max_decoder_seq_len = max_decoder_seq_len\n",
    "        self.input_data_embed = nn.Embedding(max_encoder_seq_len, self.hidden_dim)\n",
    "        self.Encoders = [Encoder(dim_num = hidden_dim) for _ in range(encoder_num)]\n",
    "        self.output_data_embed = nn.Embedding(max_decoder_seq_len, self.hidden_dim)\n",
    "        self.Decoders = [Decoder(dim_num = hidden_dim) for _ in range(decoder_num)]\n",
    "        self.last_linear_layer = nn.Linear(self.hidden_dim, max_decoder_seq_len)\n",
    "\n",
    "    def positional_encoding(self, position_max_length = 100):\n",
    "        position = torch.arange(0, position_max_length, dtype = torch.float).unsqueeze(1)\n",
    "        pe = torch.zeros(position_max_length, self.hidden_dim)\n",
    "        div_term = torch.pow(torch.ones(self.hidden_dim // 2).fill_(10000), torch.arange(0, self.hidden_dim, 2) / torch.tensor(self.hidden_dim, dtype = torch.float32))\n",
    "        pe[:, 0::2] = torch.sin(position/div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, input, output, mask):\n",
    "        input_embed = self.input_data_embed(input)\n",
    "        input_embed += self.positional_encoding(self.max_encoder_seq_len)\n",
    "        q, k, v = input_embed, input_embed, input_embed\n",
    "\n",
    "        for encoder in self.Encoders:\n",
    "            encoder_output = encoder(q, k, v)\n",
    "            q = encoder_output\n",
    "            k = encoder_output\n",
    "            v = encoder_output\n",
    "\n",
    "        output_embed = self.output_data_embed(output)\n",
    "        output += self.positional_encoding(self.max_decoder_seq_len)\n",
    "        output_embed = output_embed.masked_fill(mask.unsqueeze(-1), 0)\n",
    "        d_q, d_k, d_v = output_embed, output_embed, output_embed\n",
    "\n",
    "        for decoder in self.Decoders:\n",
    "            decoder_output = decoder(d_q, d_k, d_v, encoder_output, mask)\n",
    "            d_q = decoder_output\n",
    "            d_k = decoder_output\n",
    "            d_v = decoder_output\n",
    "\n",
    "        output = F.softmax(self.last_linear_layer(decoder_output), dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
