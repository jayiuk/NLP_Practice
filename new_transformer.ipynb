{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_model = False\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### einops 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "x = torch.randn(2, 3, 4) #Tensor shape : (2, 3, 4)\n",
    "y = rearrange(x, 'a b c -> c a b')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "y = rearrange(x, 'a b c -> (a b) c')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(6, 4)\n",
    "y = rearrange(x, '(a b) c -> a b c', a = 2, b = 3)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model = 512, n_heads = 8): #d_model : 임베딩 벡터의 차원, n_heads : 멀티 헤드 어텐션의 헤드 개수\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        #어텐션 스코어를 스케일링 하기 위한 값\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model / n_heads))\n",
    "\n",
    "\n",
    "    def froward(self, Q, K, V, mask = None):\n",
    "        Q = self.q_linear(Q) #(N, t, D) -> (N, t, D)\n",
    "        K = self.k_linear(K) #(N, t, D) -> (N, t, D)\n",
    "        V = self.v_linear(V) #(N, t, D) -> (N, t, D)\n",
    "        #멀티헤드를 위해 임베딩 차원 D를 헤드 개수 n_heads로 분할\n",
    "        Q = rearrange(Q, 'N t (h dk) -> N h t dk', h = self.n_heads) #(N, t, D) -> (N, h, t, D//h)\n",
    "        K = rearrange(K, 'N t (h dk) -> N h t dk', h = self.n_heads) #(N, t, D) -> (N, h, t, D//h)\n",
    "        V = rearrange(V, 'N t (h dk) -> N h t dk', h = self.n_heads) #(N, t, D) -> (N, h, t, D//h)\n",
    "        # 어텐션 스코어 구하기\n",
    "        attention_score = Q @ K.transpose(-2, -1) / self.scale #(N, h, t, dk) @ (N, h, dk, t) -> (N, h, t(쿼리의 길이), t(키의 길이))\n",
    "        # 패딩의 위치에 굉장히 작은 값 강제로 부여 -> 소프트맥스 적용 시에 0이 될 수 있도록\n",
    "        if mask is not None: #패딩 위치를 의미하는 인덱스들이 존재한다면\n",
    "            attention_score[mask] = -1e10\n",
    "        #에너지를 구하기 위해서는 키의 방향으로 소프트맥스를 적용\n",
    "        energy = torch.softmax(attention_score, dim = -1)\n",
    "        #에너지와 V를 곱해서 최종 어텐션 값 구함\n",
    "        attention = energy @ V #(N, h, t, t) @ (N, h, t, dk) -> (N, h, t, dk)\n",
    "        #헤드 차원을 연결해서 원래의 차원으로 되돌림\n",
    "        x = rearrange(attention, 'N h t dk -> N t (h dk)') #(N, h, t, dk) -> (N, t, D)\n",
    "        #최종 출력값에 대해 선형 변환을 적용. 각각의 헤드의 생각을 섞어줌\n",
    "        output = self.out_linear(x) #(N, t, D) -> (N, t, D)\n",
    "        return output, energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN(FeedForward Network)\n",
    "- 정확한 이름은 Position-wise Feed Forward Network\n",
    "- 인코더 디코더의 Multi Head Attention의 결과를 하나로 합쳐주는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model = 512, d_ff = 2048, drop_p = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, mha_output):\n",
    "        out = self.linear(mha_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        # Multi Head Attention Layer\n",
    "        self.self_attention = MHA(d_model, n_heads)\n",
    "        #MHA에 대한 layer normalization\n",
    "        self.atten_ln = nn.LayerNorm(d_model)\n",
    "        #Feed Forward Network\n",
    "        self.ff = FeedForward(d_model, d_ff, drop_p)\n",
    "        #Normalization\n",
    "        self.ff_ln = nn.LayerNorm(d_model)\n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x, enc_mask):\n",
    "        residual, attention_enc = self.self_attention(Q = x, K = x, V = x, mask = enc_mask)\n",
    "        residual = self.dropout(residual)\n",
    "        #skip connection & layer norm\n",
    "        encoder_self_atten_output = self.atten_ln(x + residual)\n",
    "        # FFN\n",
    "        residual = self.ff(encoder_self_atten_output)\n",
    "        residual = self.dropout(residual)\n",
    "        encoder_ffn_output = self.ff_ln(encoder_self_atten_output + residual)\n",
    "        return encoder_ffn_output, attention_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        #d_model의 제곱근 값으로 scale을 정의 -> 임베딩 벡터의 크기를 조정\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model))\n",
    "        #입력 임베딩 레이어\n",
    "        self.input_embedding = input_embedding\n",
    "        # 위치 임베딩\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        # 여러 개의 인코더 레이어를 쌓기 위해 모듈 리스트 활용\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, mask, atten_map_save = False):\n",
    "        # 위치 인덱스 텐서 생성 : 각 배치에서 시퀀스의 길이만큼 위치 인덱스를 반복\n",
    "        pos = torch.arange(src.shape[1]).repeat(src.shape[0], 1).to(self.device)\n",
    "        x_embedding = self.input_embedding(src) + self.pos_embedding(pos)\n",
    "        x_embedding = self.dropout(x_embedding)\n",
    "        atten_encs = torch.tensor([]).to(self.device)\n",
    "        for layer in self.layers:\n",
    "            encoder_output, atten_encs = layer(encoder_output, mask)\n",
    "            if atten_map_save:\n",
    "                atten_encs = torch.cat(atten_encs, atten_encs[0].unsqueeze())\n",
    "\n",
    "        return encoder_output, atten_encs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== attention_score에 마스킹 적용 전 ====================\n",
      "attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\n",
      " tensor([[-0.6378,  0.3716,  1.1410, -0.1698, -0.4231, -0.3101,  0.8877,  1.0603,\n",
      "          0.2497, -0.8980],\n",
      "        [-0.1975, -0.1490, -0.8522,  0.7071, -1.9913, -2.4263, -0.6625,  1.4701,\n",
      "         -0.1316,  0.4624],\n",
      "        [-1.0739,  0.2155, -0.0336, -0.3841,  0.5567, -0.2092, -0.9420, -0.2604,\n",
      "          1.4805,  0.1632],\n",
      "        [ 0.3468, -0.6148, -0.4828, -0.3025, -0.9346,  1.0462, -0.6604,  0.1264,\n",
      "          0.4500, -0.7462],\n",
      "        [ 0.7712,  0.8880, -0.5860,  1.6809, -0.9139, -0.3729, -1.2812,  0.4910,\n",
      "          0.2157, -2.3383],\n",
      "        [-0.5301, -0.3016, -1.4346, -0.3275,  1.1542, -0.6850, -0.3721, -0.4474,\n",
      "          0.1354,  1.2107],\n",
      "        [-0.2449, -1.2759,  1.3128,  1.8228,  0.4469, -0.4300, -1.6372, -0.9488,\n",
      "         -0.0604,  0.7848],\n",
      "        [ 0.9471, -0.3743, -0.3295,  0.0621,  1.7466,  0.2034,  0.2784,  1.2467,\n",
      "         -0.9168, -1.9631],\n",
      "        [ 0.8056, -1.3352, -1.2483,  1.3437,  0.0956, -0.5525,  0.8257,  0.3949,\n",
      "          1.1373,  0.2790],\n",
      "        [ 0.5591, -0.1312,  0.9676, -0.5425, -0.6129, -0.4094, -0.5848, -2.2052,\n",
      "         -0.9006,  1.6335]])\n",
      "==================== attention_score에 마스킹 적용 전 ====================\n",
      "attention_score shape: torch.Size([3, 8, 10, 10])\n",
      "torch.Size([3, 8, 10, 10])\n",
      "==================== attention_score에 마스킹 적용 후 ====================\n",
      "attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\n",
      " tensor([[-0.6378,  0.3716,  1.1410, -0.1698, -0.4231, -0.3101,  0.8877,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.1975, -0.1490, -0.8522,  0.7071, -1.9913, -2.4263, -0.6625,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-1.0739,  0.2155, -0.0336, -0.3841,  0.5567, -0.2092, -0.9420,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.3468, -0.6148, -0.4828, -0.3025, -0.9346,  1.0462, -0.6604,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.7712,  0.8880, -0.5860,  1.6809, -0.9139, -0.3729, -1.2812,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.5301, -0.3016, -1.4346, -0.3275,  1.1542, -0.6850, -0.3721,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.2449, -1.2759,  1.3128,  1.8228,  0.4469, -0.4300, -1.6372,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.9471, -0.3743, -0.3295,  0.0621,  1.7466,  0.2034,  0.2784,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.8056, -1.3352, -1.2483,  1.3437,  0.0956, -0.5525,  0.8257,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.5591, -0.1312,  0.9676, -0.5425, -0.6129, -0.4094, -0.5848,  0.0000,\n",
      "          0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 예제 설정\n",
    "batch_size = 3\n",
    "seq_len = 10\n",
    "padding = 3\n",
    "n_heads = 8\n",
    "\n",
    "# attention_score 텐서 생성 (무작위 값으로 초기화)\n",
    "# Shape: (batch_size, n_heads, seq_len, seq_len)\n",
    "attention_score = torch.randn(batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "# enc_mask 생성: 패딩이 있는 위치를 마스킹\n",
    "# 각 시퀀스의 마지막 3개 위치에 패딩이 있다고 가정합니다.\n",
    "enc_mask = torch.tensor([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],  # 첫 번째 시퀀스\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],  # 두 번째 시퀀스\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]   # 세 번째 시퀀스\n",
    "], dtype=torch.bool).unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# enc_mask의 shape을 (batch_size, n_heads, seq_len, seq_len)로 확장\n",
    "enc_mask = enc_mask.expand(batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "print(\"=\"*20, \"attention_score에 마스킹 적용 전\", \"=\"*20)\n",
    "print(\"attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\\n\",attention_score[0, 0])\n",
    "\n",
    "# attention_score에 enc_mask 적용\n",
    "if enc_mask is not None:\n",
    "    attention_score[enc_mask] = 0  # 마스크된 위치에 매우 작은 값을 넣어 softmax 결과에 영향을 미치지 않도록 합니다.\n",
    "\n",
    "print(\"=\"*20, \"attention_score에 마스킹 적용 전\", \"=\"*20)\n",
    "print(\"attention_score shape:\", attention_score.shape)\n",
    "print(enc_mask.shape)\n",
    "\n",
    "# 이제 attention_score 텐서에서 마스크가 적용된 부분을 확인할 수 있습니다.\n",
    "# 특정 헤드에 대한 attention_score 확인 (예: 첫 번째 헤드)\n",
    "print(\"=\"*20, \"attention_score에 마스킹 적용 후\", \"=\"*20)\n",
    "print(\"attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\\n\", attention_score[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        self.self_atten = MHA(d_model, n_heads)\n",
    "        self.self_atten_ln = nn.LayerNorm(d_model)\n",
    "        # encoder-decoder attention layer\n",
    "        self.enc_dec_atten = MHA(d_model, n_heads)\n",
    "        self.enc_dec_atten_ln = nn.LayerNorm(d_model)\n",
    "        # FeedForward\n",
    "        self.ff = FeedForward(d_model, d_ff, drop_p)\n",
    "        self.ff_ln = nn.LayerNorm(d_model)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
    "        residual, atten_dec = self.self_atten(Q = x, K = x, V = x, mask = dec_mask)\n",
    "        residual = self.dropout(residual)\n",
    "        decoder_masked_self_attention_output = self.self_atten_ln(x + residual)\n",
    "        residual, atten_dec_enc = self.enc_dec_atten(\n",
    "            Q = decoder_masked_self_attention_output,\n",
    "            K = enc_out,\n",
    "            V = enc_out,\n",
    "            mask = enc_dec_mask\n",
    "        )\n",
    "        residual = self.dropout(residual)\n",
    "        decoder_self_attention_output = self.enc_dec_atten_ln(x + residual)\n",
    "        residual = self.ff(decoder_self_attention_output)\n",
    "        residual = self.dropout(residual)\n",
    "        decoder_output = self.ff_ln(decoder_self_attention_output + residual)\n",
    "\n",
    "        return decoder_output, atten_dec, atten_dec_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p, vocab_size):\n",
    "        super().__init__()\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model))\n",
    "        self.input_embedding = input_embedding\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, trg, enc_out, dec_mask, enc_dec_mask, atten_map_save = False):\n",
    "        pos = torch.arange(trg.shape[1]).repeat(trg.shape[0], 1).to(self.device)\n",
    "        y_embedding = self.scale * self.input_embedding(trg) + self.pos_embedding(pos)\n",
    "        y_embedding = self.dropout(y_embedding)\n",
    "        atten_decs = torch.tensor([]).to(self.device)\n",
    "        atten_enc_decs = torch.tensor([]).to(self.device)\n",
    "        decoder_output = y_embedding\n",
    "        for layer in self.layers:\n",
    "            decoder_output, atten_dec, atten_enc_dec = layer(decoder_output, enc_out, dec_mask, enc_dec_mask)\n",
    "            if atten_map_save:\n",
    "                atten_decs = torch.cat([atten_decs, atten_dec[0].unsqueeze()])\n",
    "                atten_enc_decs = torch.cat([atten_enc_decs, atten_enc_dec[0].unsqueeze()])\n",
    "        decoder_output_linear = self.linear(decoder_output)\n",
    "\n",
    "        return decoder_output_linear, atten_decs, atten_enc_decs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_layers, pad_idx, d_model = 512, d_ff = 2048, n_heads = 8, drop_p = 0.1)\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = Encoder(self.input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p)\n",
    "        self.decoder = Decoder(self.input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p, vocab_size)\n",
    "        self.n_heads = n_heads\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def make_enc_mask(self, src):\n",
    "        enc_mask = (src == self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        enc_mask = enc_mask.repeat(1, self.n_heads, src.shape[1], 1)\n",
    "        return enc_mask\n",
    "    \n",
    "    def make_dec_mask(self, trg):\n",
    "        trg_pad_mask = (trg.to('cpu') == self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_pad_mask = trg_pad_mask.repeat(1, self.n_heads, trg.shape[1], 1)\n",
    "        trg_future_mask = torch.tril(torch.ones(trg.shape[0], self.n_heads, trg.shape[1], trg.shape[1])) == 0\n",
    "        dec_mask = trg_pad_mask | trg_future_mask\n",
    "        return dec_mask\n",
    "    \n",
    "    def make_enc_dec_mask(self, src, trg):\n",
    "        enc_dec_mask = (src == self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        enc_dec_mask = enc_dec_mask.repeat(1, self.n_heads, trg.shape[1], 1)\n",
    "        return enc_dec_mask\n",
    "    \n",
    "    def forward(self, src, trg):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
